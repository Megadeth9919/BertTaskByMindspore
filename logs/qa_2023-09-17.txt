[2023-09-17 11:02:55] - INFO:  ### 将当前配置打印到日志文件中 
[2023-09-17 11:02:55] - INFO: ### project_dir = D:\PyCharm 2023.1.3\PyCharm_Projects\BertForQuestionAnswerByMindspore
[2023-09-17 11:02:55] - INFO: ### dataset_dir = D:\PyCharm 2023.1.3\PyCharm_Projects\BertForQuestionAnswerByMindspore\.mindnlp\datasets\SQuAD1
[2023-09-17 11:02:55] - INFO: ### logs_save_dir = D:\PyCharm 2023.1.3\PyCharm_Projects\BertForQuestionAnswerByMindspore\logs
[2023-09-17 11:02:55] - INFO: ### pretrained_model_name = bert-base-uncased
[2023-09-17 11:02:55] - INFO: ### pretrained_model_dir = D:\PyCharm 2023.1.3\PyCharm_Projects\BertForQuestionAnswerByMindspore\.mindnlp\models\bert-base-uncased
[2023-09-17 11:02:55] - INFO: ### max_answer_len = 30
[2023-09-17 11:02:55] - INFO: ### max_query_len = 64
[2023-09-17 11:02:55] - INFO: ### max_seq_len = 384
[2023-09-17 11:02:55] - INFO: ### doc_stride = 128
[2023-09-17 11:02:55] - INFO: ### batch_size = 12
[2023-09-17 11:02:55] - INFO: ### learning_rate = 3.5e-05
[2023-09-17 11:02:55] - INFO: ### epochs = 1
[2023-09-17 11:02:55] - INFO: ### finetuning_task = None
[2023-09-17 11:02:55] - INFO: ### num_labels = 2
[2023-09-17 11:02:55] - INFO: ### output_attentions = False
[2023-09-17 11:02:55] - INFO: ### output_hidden_states = False
[2023-09-17 11:02:55] - INFO: ### is_decoder = False
[2023-09-17 11:02:55] - INFO: ### pad_token_id = 0
[2023-09-17 11:02:55] - INFO: ### eos_token_id = None
[2023-09-17 11:02:55] - INFO: ### is_encoder_decoder = False
[2023-09-17 11:02:55] - INFO: ### add_cross_attention = False
[2023-09-17 11:02:55] - INFO: ### tie_word_embeddings = True
[2023-09-17 11:02:55] - INFO: ### decoder_start_token_id = None
[2023-09-17 11:02:55] - INFO: ### return_dict = False
[2023-09-17 11:02:55] - INFO: ### chunk_size_feed_forward = 0
[2023-09-17 11:02:55] - INFO: ### pruned_heads = {}
[2023-09-17 11:02:55] - INFO: ### problem_type = None
[2023-09-17 11:02:55] - INFO: ### vocab_size = 30522
[2023-09-17 11:02:55] - INFO: ### hidden_size = 768
[2023-09-17 11:02:55] - INFO: ### num_hidden_layers = 12
[2023-09-17 11:02:55] - INFO: ### num_attention_heads = 12
[2023-09-17 11:02:55] - INFO: ### hidden_act = gelu
[2023-09-17 11:02:55] - INFO: ### intermediate_size = 3072
[2023-09-17 11:02:55] - INFO: ### hidden_dropout_prob = 0.1
[2023-09-17 11:02:55] - INFO: ### attention_probs_dropout_prob = 0.1
[2023-09-17 11:02:55] - INFO: ### max_position_embeddings = 512
[2023-09-17 11:02:55] - INFO: ### type_vocab_size = 2
[2023-09-17 11:02:55] - INFO: ### initializer_range = 0.02
[2023-09-17 11:02:55] - INFO: ### layer_norm_eps = 1e-12
[2023-09-17 11:02:55] - INFO: ### classifier_dropout = None
